{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a6afa2",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This file captures the results of different model runs and evaluates the best performance in prediction on test data after parameter tuning.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The keystroke dynamics data from 20 users are first selected and pre-processed before the learning starts.  The data are first split in 20/80 for testing / training dataset.  The data are then scaled, then trained and evaluated using cross validation accuracy scoring on taining dataset.  The trained model is then run on test dataset and compare for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import model selection and data preparation libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Import algorithm libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4fd07",
   "metadata": {},
   "source": [
    "#### Loading Data and Selection on Features and Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and extract first 20 subjects\n",
    "whole_dataset = pd.read_csv('DSL-StrongPasswordData.csv')\n",
    "\n",
    "first_20subject = (whole_dataset.groupby(by='subject', axis=0).count().index[:20])\n",
    "selected_dataset = whole_dataset[whole_dataset['subject'].isin(first_20subject)]\n",
    "\n",
    "# The DD and UD timings of each key have been showed to be highly correlated to each other \n",
    "# in a separate analysis. Hence, we will drop all features starting with 'DD'\n",
    "all_features = selected_dataset.columns[3:34]\n",
    "selected_features = [x for x in all_features if not x.startswith('DD')]\n",
    "\n",
    "# Get a copy of dataset with selected feature columns\n",
    "df = selected_dataset[selected_features].copy()\n",
    "# df['subject'] = selected_dataset['subject']\n",
    "\n",
    "# Show samples\n",
    "df.head()\n",
    "\n",
    "# Get feature columns\n",
    "#feature_data = selected_dataset[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60704131",
   "metadata": {},
   "source": [
    "#### Perform training and test data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, selected_dataset['subject'],test_size=0.20, random_state=42, stratify=selected_dataset['subject'])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e4c30",
   "metadata": {},
   "source": [
    "#### Perform scaling on feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09411a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store best performing model\n",
    "eval_results = pd.DataFrame({\"Accuracy\" : []})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac75c1",
   "metadata": {},
   "source": [
    "# A) Single Algorithm Models\n",
    "First part of this experimentation looks into the single algorithm model performance after learning from the benchmark dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Experiment to capture logs\n",
    "experiment_id = mlflow.create_experiment(\"Single Model Evaluation Experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f4aaf",
   "metadata": {},
   "source": [
    "## 1) Experiment Run on Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"DECISION TREE\"\n",
    "with mlflow.start_run(run_name=Model_Name,\n",
    "                      experiment_id=experiment_id,\n",
    "                      tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                      description=\"A run on different MAX_DEPTH settings\") as run:\n",
    "    max_depth = [5, 10, 20, 30, 40, 50, 60]\n",
    "    best_score = 0\n",
    "    \n",
    "    \n",
    "    for n in max_depth:\n",
    "        with mlflow.start_run(nested=True,\n",
    "                              experiment_id=experiment_id,\n",
    "                              run_name=\"CHILD MAX_DEPTH \" + str(n)):\n",
    "            \n",
    "            # Logging parameters\n",
    "            mlflow.log_param(\"max_depth\", n)\n",
    "            mlflow.log_param(\"max_leaf_nodes\", \"Unlimited\")\n",
    "            \n",
    "            # Model instantiation\n",
    "            clf = DecisionTreeClassifier(random_state=42,  max_depth=n)\n",
    "            scores = cross_val_score(clf, X_train_scaled, y_train, cv=10)\n",
    "            \n",
    "             # logging metrics\n",
    "            mlflow.log_metric(\"training_score\", scores.mean())\n",
    "            \n",
    "            # Scoring on the test data set\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            test_score = clf.score(X_test_scaled, y_test)\n",
    "            mlflow.log_metric(\"test_score\", test_score)\n",
    "            if test_score > best_score:\n",
    "                best_score = test_score\n",
    "\n",
    "# Capture best score\n",
    "eval_results.loc[Model_Name] = {\"Accuracy\": best_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c602de6d",
   "metadata": {},
   "source": [
    "## 2) Experiment Run On K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"K-NEAREST NEIGHBOUR\"\n",
    "with mlflow.start_run(run_name=Model_Name,\n",
    "                      experiment_id=experiment_id,\n",
    "                      tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                      description=\"A run on different K values\") as run:\n",
    "    knn = np.arange(16, dtype=int) + 1\n",
    "    best_score = 0\n",
    "    \n",
    "    for k in knn:\n",
    "        with mlflow.start_run(nested=True,\n",
    "                              experiment_id=experiment_id,\n",
    "                              run_name=\"CHILD K = \" + str(k)):\n",
    "            \n",
    "            # Logging parameters\n",
    "            mlflow.log_param(\"k\", k)\n",
    "            \n",
    "            # Model instantiation\n",
    "            clf = KNeighborsClassifier( n_neighbors=k)\n",
    "            \n",
    "            # Measure performance via cross validation fold\n",
    "            scores = cross_val_score(clf, X_train_scaled, y_train, cv=10)\n",
    "            \n",
    "             # logging metrics\n",
    "            mlflow.log_metric(\"training_score\", scores.mean())\n",
    "            \n",
    "            # Scoring on the test data set\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            test_score = clf.score(X_test_scaled, y_test)\n",
    "            \n",
    "            # Logging test score\n",
    "            mlflow.log_metric(\"test_score\", test_score)\n",
    "            if test_score > best_score:\n",
    "                best_score = test_score\n",
    "\n",
    "# Capture best score\n",
    "eval_results.loc[Model_Name] = {\"Accuracy\": best_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b66599",
   "metadata": {},
   "source": [
    "## 3) Experiment Run On Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da964650",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"NAIVE BAYES\"\n",
    "with mlflow.start_run(run_name=Model_Name,\n",
    "                      experiment_id=experiment_id,\n",
    "                      tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                      description=\"A run on different var_smoothing\") as run:\n",
    "    smoothing_var = [1e-10, 1e-9, 1e-8, 1e-7, 1e-6]\n",
    "    best_score = 0\n",
    "    \n",
    "    for s in smoothing_var:\n",
    "        with mlflow.start_run(nested=True,\n",
    "                              experiment_id=experiment_id,\n",
    "                              run_name=\"CHILD smoothing_var = \" + str(s)):\n",
    "            \n",
    "            # Logging parameters\n",
    "            mlflow.log_param(\"smoothing\", s)\n",
    "            \n",
    "            # Model instantiation\n",
    "            clf = GaussianNB(var_smoothing=s)\n",
    "            \n",
    "            # Measure performance via cross validation fold\n",
    "            scores = cross_val_score(clf, X_train_scaled, y_train, cv=10)\n",
    "            \n",
    "             # logging metrics\n",
    "            mlflow.log_metric(\"training_score\", scores.mean())\n",
    "            \n",
    "            # Scoring on the test data set\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            test_score = clf.score(X_test_scaled, y_test)\n",
    "            \n",
    "            # Logging test score\n",
    "            mlflow.log_metric(\"test_score\", test_score)\n",
    "            if test_score > best_score:\n",
    "                best_score = test_score\n",
    "\n",
    "# Capture best score\n",
    "eval_results.loc[Model_Name] = {\"Accuracy\": best_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b5688",
   "metadata": {},
   "source": [
    "## 4) Experiment Run On Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"LOGISTIC REGRESSION\"\n",
    "with mlflow.start_run(run_name=Model_Name,\n",
    "                      experiment_id=experiment_id,\n",
    "                      tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                      description=\"A run on different Regularisation Strength C\") as run:\n",
    "    \n",
    "    \n",
    "    \n",
    "    C = [1e-3, 1e-2, 0.1, 1, 10, 100, 1000]\n",
    "    best_score = 0\n",
    "    \n",
    "    for r in C:\n",
    "        with mlflow.start_run(nested=True,\n",
    "                              experiment_id=experiment_id,\n",
    "                              run_name=\"CHILD regularization strength = \" + str(r)):\n",
    "            \n",
    "            # Logging parameters\n",
    "            mlflow.log_param(\"C\", r)\n",
    "            \n",
    "            # Model instantiation\n",
    "            clf = LogisticRegression(random_state=42, C=r, max_iter=5000)\n",
    "            \n",
    "            # Measure performance via cross validation fold\n",
    "            scores = cross_val_score(clf, X_train_scaled, y_train, cv=10)\n",
    "            \n",
    "             # logging metrics\n",
    "            mlflow.log_metric(\"training_score\", scores.mean())\n",
    "            \n",
    "            # Scoring on the test data set\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            test_score = clf.score(X_test_scaled, y_test)\n",
    "            \n",
    "            # Logging test score\n",
    "            mlflow.log_metric(\"test_score\", test_score)\n",
    "            if test_score > best_score:\n",
    "                best_score = test_score\n",
    "\n",
    "# Capture best score\n",
    "eval_results.loc[Model_Name] = {\"Accuracy\": best_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b449a03",
   "metadata": {},
   "source": [
    "## 5) Experiment Run On Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f38462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Model_Name = \"SUPPORT VECTOR MACHINE\"\n",
    "with mlflow.start_run(run_name=Model_Name,\n",
    "                      experiment_id=experiment_id,\n",
    "                      tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                      description=\"A run on different Gamma and Regularisation\") as run:\n",
    "    \n",
    "    \n",
    "    \n",
    "    C = [1e-3, 1e-2, 0.1, 1, 10, 100]\n",
    "    gamma = [0.01, 0.1, 1, 5]\n",
    "    best_score = 0\n",
    "    \n",
    "    for r in C:\n",
    "        for g in gamma:\n",
    "            with mlflow.start_run(nested=True,\n",
    "                              experiment_id=experiment_id,\n",
    "                              run_name=\"CHILD C = \" + str(r) + \", GAMMA =\" + str(g)):\n",
    "            \n",
    "                # Logging parameters\n",
    "                mlflow.log_param(\"C\", r)\n",
    "                mlflow.log_param(\"Gamma\", g)                  \n",
    "\n",
    "                # Model instantiation\n",
    "                clf = SVC(random_state=42, C=r, gamma=g, kernel='rbf')\n",
    "                \n",
    "                # Measure performance via cross validation fold\n",
    "                scores = cross_val_score(clf, X_train_scaled, y_train, cv=10)\n",
    "\n",
    "                 # logging metrics\n",
    "                mlflow.log_metric(\"training_score\", scores.mean())\n",
    "\n",
    "                # Scoring on the test data set\n",
    "                clf.fit(X_train_scaled, y_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                test_score = clf.score(X_test_scaled, y_test)\n",
    "                \n",
    "                # Logging test score\n",
    "                mlflow.log_metric(\"test_score\", test_score)\n",
    "                if test_score > best_score:\n",
    "                    best_score = test_score\n",
    "\n",
    "# Capture best score\n",
    "eval_results.loc[Model_Name] = {\"Accuracy\": best_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8ba90",
   "metadata": {},
   "source": [
    "# B) Ensemble Algorithm Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d21b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new experiment ID to capture logs for ensemble\n",
    "ensemble_id = mlflow.create_experiment(\"Ensemble Model Evaluation Experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1bd647",
   "metadata": {},
   "source": [
    "## 1) Experimental Run On Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"RANDOM FOREST\"\n",
    "with mlflow.start_run(run_name=Model_Name,\n",
    "                      experiment_id=ensemble_id,\n",
    "                      tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                      description=\"A run on different estimator count and max depth\") as run:\n",
    "    \n",
    "    estimator_count = [50, 100, 200, 300, 400]\n",
    "    max_depth = [3, 4, 5, 6]\n",
    "    best_score = 0\n",
    "    \n",
    "    for e in estimator_count:\n",
    "        for d in max_depth:\n",
    "            with mlflow.start_run(nested=True,\n",
    "                              experiment_id=ensemble_id,\n",
    "                              run_name=\"CHILD n_est = \" + str(e) + \", max_dep =\" + str(d)):\n",
    "                \n",
    "                # Logging parameters\n",
    "                mlflow.log_param(\"n_estimators\", e)\n",
    "                mlflow.log_param(\"max_depth\", d)\n",
    "                \n",
    "                # Model instantiation\n",
    "                clf = RandomForestClassifier(random_state=42, n_estimators=e, max_depth=d)\n",
    "                \n",
    "                # Measure performance via cross validation fold\n",
    "                scores = cross_val_score(clf, X_train_scaled, y_train, cv=10)\n",
    "\n",
    "                 # logging metrics\n",
    "                mlflow.log_metric(\"training_score\", scores.mean())\n",
    "\n",
    "                # Scoring on the test data set\n",
    "                clf.fit(X_train_scaled, y_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                test_score = clf.score(X_test_scaled, y_test)\n",
    "                \n",
    "                # Logging test score\n",
    "                mlflow.log_metric(\"test_score\", test_score)\n",
    "                if test_score > best_score:\n",
    "                    best_score = test_score\n",
    "\n",
    "                    \n",
    "# Capture best score\n",
    "eval_results.loc[Model_Name] = {\"Accuracy\": best_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09363b55",
   "metadata": {},
   "source": [
    "## 2) Experiment Run On Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b17f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"GRADIENT BOOSTING\"\n",
    "with mlflow.start_run(run_name=Model_Name,\n",
    "                      experiment_id=ensemble_id,\n",
    "                      tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                      description=\"A run on different estimator count and learning rate\") as run:\n",
    "    \n",
    "    estimator_count = [50, 100, 200, 300, 400]\n",
    "    learning_rate = [0.1, 1, 10]\n",
    "    best_score = 0\n",
    "    \n",
    "    for e in estimator_count:\n",
    "        for l in learning_rate:\n",
    "            with mlflow.start_run(nested=True,\n",
    "                              experiment_id=ensemble_id,\n",
    "                              run_name=\"CHILD n_est = \" + str(e) + \", learn rate =\" + str(l)):\n",
    "                \n",
    "                # Logging parameters\n",
    "                mlflow.log_param(\"n_estimators\", e)\n",
    "                mlflow.log_param(\"learn_rate\", l)\n",
    "                \n",
    "                # Model instantiation\n",
    "                clf = GradientBoostingClassifier(random_state=42, n_estimators=e, learning_rate=l)\n",
    "                \n",
    "                # Measure performance via cross validation fold\n",
    "                scores = cross_val_score(clf, X_train_scaled, y_train, cv=10)\n",
    "\n",
    "                 # logging metrics\n",
    "                mlflow.log_metric(\"training_score\", scores.mean())\n",
    "\n",
    "                # Scoring on the test data set\n",
    "                clf.fit(X_train_scaled, y_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                test_score = clf.score(X_test_scaled, y_test)\n",
    "                \n",
    "                # Logging test score\n",
    "                mlflow.log_metric(\"test_score\", test_score)\n",
    "                if test_score > best_score:\n",
    "                    best_score = test_score\n",
    "\n",
    "                    \n",
    "# Capture best score\n",
    "eval_results.loc[Model_Name] = {\"Accuracy\": best_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd955c1",
   "metadata": {},
   "source": [
    "## 3) Experiment Run On Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name = \"VOTING ENSEMBLE\"\n",
    "with mlflow.start_run(run_name=Model_Name,\n",
    "                      experiment_id=ensemble_id,\n",
    "                      tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                      description=\"A run on Voting ensemble with 3 different base estimator\") as run:\n",
    "    \n",
    "    # Define base estimators\n",
    "    clf1 = LogisticRegression(random_state=42, max_iter=5000)\n",
    "    clf2 = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    clf3 = GaussianNB()\n",
    "    \n",
    "    voting_type = ['hard', 'soft']\n",
    "    for v in voting_type:\n",
    "        with mlflow.start_run(nested=True,\n",
    "                              experiment_id=ensemble_id,\n",
    "                              run_name=\"CHILD voting = \" + v):\n",
    "            \n",
    "            # Logging parameters\n",
    "            mlflow.log_param(\"voting_type\", v)\n",
    "#             mlflow.log_param(\"learn_rate\", l)\n",
    "                \n",
    "            # Model instantiation\n",
    "            clf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),('gnb', clf3)], voting=v)\n",
    "        \n",
    "            # Measure performance via cross validation fold\n",
    "            scores = cross_val_score(clf, X_train_scaled, y_train, cv=10)\n",
    "\n",
    "            # logging metrics\n",
    "            mlflow.log_metric(\"training_score\", scores.mean())\n",
    "\n",
    "            # Scoring on the test data set\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            test_score = clf.score(X_test_scaled, y_test)\n",
    "\n",
    "            # Logging test score\n",
    "            mlflow.log_metric(\"test_score\", test_score)\n",
    "            if test_score > best_score:\n",
    "                best_score = test_score\n",
    "\n",
    "                    \n",
    "# Capture best score\n",
    "eval_results.loc[Model_Name] = {\"Accuracy\": best_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78632beb",
   "metadata": {},
   "source": [
    "# Comparison of Accuracies on Test Dataset\n",
    "Finally, all the best accuracy score from each algorithm is collated and plotted below side-by-side for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf78cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting best score accuracy vs models\n",
    "print(eval_results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "c = ['red', 'yellow', 'orange', 'green', 'blue', 'purple', 'black', 'brown']\n",
    "\n",
    "ax.set_ylabel('Prediction Accuracy')\n",
    "ax.set_xlabel('Accuracy on Test Dataset')\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "ax.bar(eval_results.index, eval_results['Accuracy'], color=c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
